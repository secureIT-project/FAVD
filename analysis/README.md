
[[part of the FAVD replication package]](https://github.com/secureIT-project/FAVD)

# Data Analysis Steps

Note that the directory [`used`](used) contains a copy of the CSV files as they were 
collected and used for the paper. The commands below describe how to regenerate those 
CSV files from the intermediate data that is generated in Step 2, and how to analyze 
those CSV files using the R scripts provided.


## for RQ1

This data is generated directly by the script `../dangerous-words/AllDangerous.py`


## for RQ2 
The RQ2 data includes only that generated by FAVD_L.

    (cd ../dangerous-words/ ; extract data-m/* ; mv .data.csv  ../analysis/used/data-m.csv)
    rq2-m-only.R used/data-m.csv


## for RQ3
Two data sets are used with RQ3.  The first uses fixed weights.  It also includes the best of the `data-m` data for comparison.
 
    (cd ../dangerous-words/; extract data-weights/* ; mv .data.csv ../analysis/weights.csv ; \
    extract data-m/*-m.0.00* data-m/*-m.0.05* data-m/*-m.0.90* data-m/*-m.0.99,* ; \
    grep -v "program,model" .data.csv >> ../analysis/weights.csv)
    rq3.R weights.csv
    rq3-stats.R weights.csv


## Data for Figure 2-4 graphs

`individual.R` builds the ROC, PRC, and other graphs using the data from a single data set generated when the following line is uncommented in `DW_Common.py`: 

    # model_search(dangerous, test, name, plus, minus)     # also generates data for graphs

The raw data files must first be separated.  For example,

    separate ../dangerous-words/data-w-details/FFmpeg.-c.-999,0,0.raw 
    individual.R FFmpeg.-c.-999,0,0.csv

The graphs used in the paper are on page two of the resulting pdf file and can be extracted with

    pdfseparate -f 2 -l 2 graphs.pdf graphs%d.pdf

By construction ROC curves all end at the point (1.00, 1.00).  It is often visually advantageous to suppress the final line segment to (1.00, 1.00) from the graph.  This can be done in `individual.R` by uncommenting the line after `# suppress lines to 1.00,1.00`.

The directory [`used`](used) includes the data used to generate the graphs for Figures 2-4:

    individual.R b FFmpeg.-c.-999,0,0.csv
    individual.R e LibPNG.-c.-999,0,0.csv
    individual.R f LibPNG.-c.0.00,0,0.csv
    individual.R g loo.-c.-999,0,0.csv
    individual.R h VDISC.-c.-999,10,1.csv


##  RQ3b

The following use the algorithm-selected plus and minus values unlike `data-weights/*`.   The data again includes the best of the `-m` data for comparison. 

    set ms="data-m/*-m.0.00,x,x.raw data-m/*-m.0.05,x,x.raw data-m/*-m.0.90,x,x.raw data-m/*-m.0.99,x,x.raw"
    (cd ../dangerous-words/ ; extract data-c/* $ms ; mv .data.csv  ../analysis/data-c.csv)
    rq3b.R data-c.csv
    rq3b-stats.R data-c.csv
    
Uncomment these two lines in `rq3b-stats.R` to use best possible value:

    # d$predicted <- d$best.test
    # cat("*** best test data ....\n")


## Fold check

To perform the preliminary fold check replace `folds = 5` in `CrossValidation.py` with `folds = 10` and then regenerate all the data.  After extraction `fold-cmp.R` addresses the question "do more folds help?"  The answer was "no".

    fold-cmp.R 5-fold.csv 10-fold.csv


## Future Work

###  Alternative find algorithms

In additoin to the find algorithm used in the paper, `DW_Common.py` includes both an old binary search algorithm `old_find_best_K_n_threshold` and the start of potentially faster search `new_find_best_K_n_threshold`. To use one of these update `DW_Common.py` and then regenerate the data to `data-new`.

    (cd ../dangerous-words/ ; extract data-new/*   ; mv .data.csv ../analysis/new.csv)
    search-cmp.R new.csv data-c.csv


### Compare various subsets of the six programs (see `f*` in the scripts `doc` and `dom`)

    extract data-loo/* data-m/loo.-m.0.05,x,x.raw data-m/loo.-m.0.90,x,x.raw data-m/loo.-m.0.99,x,x.raw data-c/loo.-c.*
    loo-subsets.R loo.csv

